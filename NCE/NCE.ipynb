{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9246c07",
   "metadata": {},
   "source": [
    "The class-conditional probability $p(.|C=1)$ is modeled with $p_m(u;\\theta)$,\n",
    "thus $p(u|C=1)=p_m(u;\\theta), p(u|C=0)=p_n(u)$,\n",
    "Suppose $p(C=1)=p(C=0)=0.5$,\n",
    "$$\n",
    "p(C=1|u;\\theta)=\\frac{p_m(u;\\theta)}{p_m(u;\\theta)+p_n(u)}=h(u;\\theta)\\\\\n",
    "p(C=0|u;\\theta)=1-h(u;\\theta).\n",
    "$$\n",
    "Log-likelihood of $\\theta$:\n",
    "$$\n",
    "l(\\theta)=\\sum_t \\ln [h(x_t;\\theta)]+ \\ln [1-h(y_t;\\theta)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca979bd",
   "metadata": {},
   "source": [
    "\n",
    "$$h(u;\\theta)=\\frac{1}{1+\\frac{p_n(u)}{p_m(u;\\theta)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91dc339",
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "class NCELoss(nn.Module):\n",
    "    \"\"\"Noise Contrastive Estimation\n",
    "\n",
    "    NCE is to eliminate the computational cost of softmax\n",
    "    normalization.\n",
    "\n",
    "    There are 3 loss modes in this NCELoss module:\n",
    "        - nce: enable the NCE approximation\n",
    "        - sampled: enabled sampled softmax approximation\n",
    "        - full: use the original cross entropy as default loss\n",
    "    They can be switched by directly setting `nce.loss_type = 'nce'`.\n",
    "\n",
    "    Ref:\n",
    "        X.Chen etal Recurrent neural network language\n",
    "        model training with noise contrastive estimation\n",
    "        for speech recognition\n",
    "        https://core.ac.uk/download/pdf/42338485.pdf\n",
    "\n",
    "    Attributes:\n",
    "        noise: the distribution of noise\n",
    "        noise_ratio: $\\frac{#noises}{#real data samples}$ (k in paper)\n",
    "        norm_term: the normalization term (lnZ in paper), can be heuristically\n",
    "        determined by the number of classes, plz refer to the code.\n",
    "        reduction: reduce methods, same with pytorch's loss framework, 'none',\n",
    "        'elementwise_mean' and 'sum' are supported.\n",
    "        loss_type: loss type of this module, currently 'full', 'sampled', 'nce'\n",
    "        are supported\n",
    "\n",
    "    Shape:\n",
    "        - noise: :math:`(V)` where `V = vocabulary size`\n",
    "        - target: :math:`(B, N)`\n",
    "        - loss: a scalar loss by default, :math:`(B, N)` if `reduction='none'`\n",
    "\n",
    "    Input:\n",
    "        target: the supervised training label.\n",
    "        args&kwargs: extra arguments passed to underlying index module\n",
    "\n",
    "    Return:\n",
    "        loss: if `reduction='sum' or 'elementwise_mean'` the scalar NCELoss ready for backward,\n",
    "        else the loss matrix for every individual targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 noise,\n",
    "                 noise_ratio=100,\n",
    "                 norm_term='auto',\n",
    "                 per_word=False,\n",
    "                 ):\n",
    "        super(NCELoss, self).__init__()\n",
    "\n",
    "        # Re-norm the given noise frequency list and compensate words with\n",
    "        # extremely low prob for numeric stability\n",
    "        probs = noise / noise.sum()\n",
    "        probs = probs.clamp(min=BACKOFF_PROB)\n",
    "        renormed_probs = probs / probs.sum()\n",
    "\n",
    "        self.register_buffer('logprob_noise', renormed_probs.log())\n",
    "        self.alias = AliasMultinomial(renormed_probs)\n",
    "\n",
    "        self.noise_ratio = noise_ratio\n",
    "        if norm_term == 'auto':\n",
    "            self.norm_term = math.log(noise.numel())\n",
    "        else:\n",
    "            self.norm_term = norm_term\n",
    "        self.per_word = per_word\n",
    "        self.bce_with_logits = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, target, *args, **kwargs):\n",
    "        \"\"\"compute the loss with output and the desired target\n",
    "\n",
    "        The `forward` is the same among all NCELoss submodules, it\n",
    "        takes care of generating noises and calculating the loss\n",
    "        given target and noise scores.\n",
    "        \"\"\"\n",
    "\n",
    "        batch = target.size(0)\n",
    "        max_len = target.size(1)\n",
    "\n",
    "        noise_samples = self.get_noise(batch, max_len)\n",
    "\n",
    "        # B,N,Nr\n",
    "        logit_noise_in_noise = self.logprob_noise[noise_samples.data.view(-1)].view_as(noise_samples)\n",
    "        logit_target_in_noise = self.logprob_noise[target.data.view(-1)].view_as(target)\n",
    "\n",
    "        # (B,N), (B,N,Nr)\n",
    "        logit_target_in_model, logit_noise_in_model = self._get_logit(target, noise_samples, *args, **kwargs)\n",
    "\n",
    "        if self.training:\n",
    "            loss = self.nce_loss(\n",
    "                logit_target_in_model, logit_noise_in_model,\n",
    "                logit_noise_in_noise, logit_target_in_noise,\n",
    "            )\n",
    "        else:\n",
    "            # directly output the approximated posterior\n",
    "            loss = - logit_target_in_model\n",
    "        return loss.mean()\n",
    "\n",
    "    def get_noise(self, batch_size, max_len):\n",
    "        \"\"\"Generate noise samples from noise distribution\"\"\"\n",
    "\n",
    "        noise_size = (batch_size, max_len, self.noise_ratio)\n",
    "        if self.per_word:\n",
    "            noise_samples = self.alias.draw(*noise_size)\n",
    "        else:\n",
    "            noise_samples = self.alias.draw(1, 1, self.noise_ratio).expand(*noise_size)\n",
    "\n",
    "        noise_samples = noise_samples.contiguous()\n",
    "        return noise_samples\n",
    "\n",
    "    def _get_logit(self, target_idx, noise_idx, *args, **kwargs):\n",
    "        \"\"\"Get the logits of NCE estimated probability for target and noise\n",
    "\n",
    "        Both NCE and sampled softmax Loss are unchanged when the probabilities are scaled\n",
    "        evenly, here we subtract the maximum value as in softmax, for numeric stability.\n",
    "\n",
    "        Shape:\n",
    "            - Target_idx: :math:`(N)`\n",
    "            - Noise_idx: :math:`(N, N_r)` where `N_r = noise ratio`\n",
    "        \"\"\"\n",
    "\n",
    "        target_logit, noise_logit = self.get_score(target_idx, noise_idx, *args, **kwargs)\n",
    "\n",
    "        target_logit = target_logit.sub(self.norm_term)\n",
    "        noise_logit = noise_logit.sub(self.norm_term)\n",
    "        return target_logit, noise_logit\n",
    "\n",
    "    def get_score(self, target_idx, noise_idx, *args, **kwargs):\n",
    "        \"\"\"Get the target and noise score\n",
    "\n",
    "        Usually logits are used as score.\n",
    "        This method should be override by inherit classes\n",
    "\n",
    "        Returns:\n",
    "            - target_score: real valued score for each target index\n",
    "            - noise_score: real valued score for each noise index\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def nce_loss(self, logit_target_in_model, logit_noise_in_model, logit_noise_in_noise, logit_target_in_noise):\n",
    "        \"\"\"Compute the classification loss given all four probabilities\n",
    "\n",
    "        Args:\n",
    "            - logit_target_in_model: logit of target words given by the model (RNN)\n",
    "            - logit_noise_in_model: logit of noise words given by the model\n",
    "            - logit_noise_in_noise: logit of noise words given by the noise distribution\n",
    "            - logit_target_in_noise: logit of target words given by the noise distribution\n",
    "\n",
    "        Returns:\n",
    "            - loss: a mis-classification loss for every single case\n",
    "        \"\"\"\n",
    "\n",
    "        # NOTE: prob <= 1 is not guaranteed\n",
    "        logit_model = torch.cat([logit_target_in_model.unsqueeze(2), logit_noise_in_model], dim=2)\n",
    "        logit_noise = torch.cat([logit_target_in_noise.unsqueeze(2), logit_noise_in_noise], dim=2)\n",
    "\n",
    "        # predicted probability of the word comes from true data distribution\n",
    "        # The posterior can be computed as following\n",
    "        # p_true = logit_model.exp() / (logit_model.exp() + self.noise_ratio * logit_noise.exp())\n",
    "        # For numeric stability we compute the logits of true label and\n",
    "        # directly use bce_with_logits.\n",
    "        # Ref https://pytorch.org/docs/stable/nn.html?highlight=bce#torch.nn.BCEWithLogitsLoss\n",
    "        logit_true = logit_model - logit_noise - math.log(self.noise_ratio)\n",
    "\n",
    "        label = torch.zeros_like(logit_model)\n",
    "        label[:, :, 0] = 1\n",
    "\n",
    "        loss = self.bce_with_logits(logit_true, label).sum(dim=2)\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
