{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37135a0d",
   "metadata": {},
   "source": [
    "## FaceNet:\n",
    "* Design a harmonic embedding and a harmonic triplet loss.\n",
    "\n",
    "* Instead of encouraging all faces of one identity to be projected onto a single point, the triplet loss, however, tries to enforce a margin between each pair of faces from one person to other faces. This allows the faces for one identity to live on a manifold.\n",
    "\n",
    "### Triplet loss:\n",
    "\n",
    "$$\\sum_{i=1}^N \\left[\\|f(x_i^a)-f(x_i^p)\\|^2-\\|f(x_i^a)-f(x_i^n)\\|^2+\\alpha\\right]_{+}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea334500",
   "metadata": {},
   "source": [
    "## Triplet selection:\n",
    "\n",
    "Hard positive $x_i^p$: $\\arg\\max_{x_i^p} \\|f(x_i^a)-f(x_i^p)\\|_2^2.$\n",
    "\n",
    "Hard negative $x_i^n$: $\\arg\\min_{x_i^n} \\|f(x_i^a)-f(x_i^n)\\|_2^2.$\n",
    "\n",
    "Infeasible to compute the $\\arg\\min$ and $\\arg\\max$ across the whole training set.\n",
    "\n",
    "Generate triplets offline every $n$ steps.\n",
    "Generate triplets online within a mini-batch.\n",
    "\n",
    "Instead of picking the hardest postive, we use all anchor-positive pairs in a mini-batch while still selecting the hard negatives.\n",
    "\n",
    "Selecting the hardest negatives will lead to bad local minima early in training, specifically it can result in collapsed model (e.g., f(x)=0), [personal comment: analogy to gan training]\n",
    "\n",
    "--> select semi-hard negatives:\n",
    "$\\|f(x_i^a)-f(x_i^p)\\|_2^2<\\|f(x_i^a)-f(x_i^n)\\|_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96c980",
   "metadata": {},
   "source": [
    "## Some thoughts:\n",
    "\n",
    "why all the false accept are male whilst most of the false reject are women?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb4dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acce02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.FloatTensor(range(10)).view(2,5)\n",
    "pdist_matrix=pairwise_distance_torch(x,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc50cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix=torch.tensor([[0.,100.,125.,],[100.,0.,90.,],[125.,90.,0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6b62653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix_tile = pdist_matrix.repeat(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2824023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0., 100., 125.],\n",
       "        [100.,   0.,  90.],\n",
       "        [125.,  90.,   0.],\n",
       "        [  0., 100., 125.],\n",
       "        [100.,   0.,  90.],\n",
       "        [125.,  90.,   0.],\n",
       "        [  0., 100., 125.],\n",
       "        [100.,   0.,  90.],\n",
       "        [125.,  90.,   0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdist_matrix_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aee4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose_reshape = pdist_matrix.transpose(0, 1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25910d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.],\n",
       "        [100.],\n",
       "        [125.],\n",
       "        [100.],\n",
       "        [  0.],\n",
       "        [ 90.],\n",
       "        [125.],\n",
       "        [ 90.],\n",
       "        [  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "085296b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False,  True],\n",
       "        [ True, False,  True],\n",
       "        [ True, False, False],\n",
       "        [False, False, False],\n",
       "        [ True, False, False],\n",
       "        [ True,  True, False]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater = pdist_matrix_tile > transpose_reshape\n",
    "greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375d91a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def pairwise_distance_torch(embeddings, device):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "    Args:\n",
    "      embeddings: 2-D Tensor of size [number of data, feature dimension].\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    # pairwise distance matrix with precise embeddings\n",
    "    precise_embeddings = embeddings.to(dtype=torch.float32)\n",
    "    c1 = torch.pow(precise_embeddings, 2).sum(axis=-1)\n",
    "    c2 = torch.pow(precise_embeddings.transpose(0, 1), 2).sum(axis=0)\n",
    "    c3 = precise_embeddings @ precise_embeddings.transpose(0, 1)\n",
    "    c1 = c1.reshape((c1.shape[0], 1))\n",
    "    c2 = c2.reshape((1, c2.shape[0]))\n",
    "    c12 = c1 + c2\n",
    "    pairwise_distances_squared = c12 - 2.0 * c3\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = torch.max(pairwise_distances_squared, torch.tensor([0.]).to(device))\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = pairwise_distances_squared.clone()\n",
    "    error_mask[error_mask > 0.0] = 1.\n",
    "    error_mask[error_mask <= 0.0] = 0.\n",
    "\n",
    "    pairwise_distances = torch.mul(pairwise_distances_squared, error_mask)\n",
    "\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = torch.ones((pairwise_distances.shape[0], pairwise_distances.shape[1])) - torch.diag(torch.ones(pairwise_distances.shape[0]))\n",
    "    pairwise_distances = torch.mul(pairwise_distances.to(device), mask_offdiagonals.to(device))\n",
    "    return pairwise_distances\n",
    "\n",
    "def TripletSemiHardLoss(y_true, y_pred, device, margin=1.0):\n",
    "    \"\"\"Computes the triplet loss_functions with semi-hard negative mining.\n",
    "       The loss_functions encourages the positive distances (between a pair of embeddings\n",
    "       with the same labels) to be smaller than the minimum negative distance\n",
    "       among which are at least greater than the positive distance plus the\n",
    "       margin constant (called semi-hard negative) in the mini-batch.\n",
    "       If no such negative exists, uses the largest negative distance instead.\n",
    "       See: https://arxiv.org/abs/1503.03832.\n",
    "       We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
    "       [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n",
    "       2-D float `Tensor` of l2 normalized embedding vectors.\n",
    "       Args:\n",
    "         margin: Float, margin term in the loss_functions definition. Default value is 1.0.\n",
    "         name: Optional name for the op.\n",
    "       \"\"\"\n",
    "    labels, embeddings = y_true, y_pred\n",
    "\n",
    "    # Reshape label tensor to [batch_size, 1].\n",
    "    lshape = labels.shape\n",
    "    labels = torch.reshape(labels, [lshape[0], 1])\n",
    "    pdist_matrix = pairwise_distance_torch(embeddings, device)\n",
    "\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = torch.eq(labels, labels.transpose(0, 1))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = adjacency.logical_not()\n",
    "\n",
    "    batch_size = labels.shape[0]\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = pdist_matrix.repeat(batch_size, 1)\n",
    "    adjacency_not_tile = adjacency_not.repeat(batch_size, 1)\n",
    "    transpose_reshape = pdist_matrix.transpose(0, 1).reshape(-1, 1)\n",
    "    greater = pdist_matrix_tile > transpose_reshape # x_kj>x_ij for all k\n",
    "    mask = adjacency_not_tile & greater # label k \\neq label i\n",
    "\n",
    "    # final mask\n",
    "    mask_step = mask.to(dtype=torch.float32)\n",
    "    mask_step = mask_step.sum(axis=1)\n",
    "    mask_step = mask_step > 0.0\n",
    "    mask_final = mask_step.reshape(batch_size, batch_size)\n",
    "    mask_final = mask_final.transpose(0, 1)\n",
    "\n",
    "    adjacency_not = adjacency_not.to(dtype=torch.float32)\n",
    "    mask = mask.to(dtype=torch.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    axis_maximums = torch.max(pdist_matrix_tile, dim=1, keepdim=True)\n",
    "    masked_minimums = torch.min(torch.mul(pdist_matrix_tile - axis_maximums[0], mask), dim=1, keepdim=True)[0] + axis_maximums[0]\n",
    "    negatives_outside = masked_minimums.reshape([batch_size, batch_size])\n",
    "    negatives_outside = negatives_outside.transpose(0, 1)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    axis_minimums = torch.min(pdist_matrix, dim=1, keepdim=True)\n",
    "    masked_maximums = torch.max(torch.mul(pdist_matrix - axis_minimums[0], adjacency_not), dim=1, keepdim=True)[0] + axis_minimums[0]\n",
    "    negatives_inside = masked_maximums.repeat(1, batch_size)\n",
    "    semi_hard_negatives = torch.where(mask_final, negatives_outside, negatives_inside)\n",
    "    loss_mat = margin + pdist_matrix - semi_hard_negatives\n",
    "    mask_positives = adjacency.to(dtype=torch.float32) - torch.diag(torch.ones(batch_size)).to(device)\n",
    "    num_positives = mask_positives.sum()\n",
    "    triplet_loss = (torch.max(torch.mul(loss_mat, mask_positives), torch.tensor([0.]).to(device))).sum() / num_positives\n",
    "    triplet_loss = triplet_loss.to(dtype=embeddings.dtype)\n",
    "    return triplet_loss\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target, **kwargs):\n",
    "        return TripletSemiHardLoss(target, input, self.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
