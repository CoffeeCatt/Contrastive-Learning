{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DV Representation:\n",
    "\n",
    "$$D_{KL}(P||Q)=\\sup_{T:\\Omega\\rightarrow \\mathbb{R}} \\mathbb{E}_{P}[T]-\\log (\\mathbb{E}_{Q}[e^T])$$\n",
    "The bound is tight for optimal functions $T^*$ that relate the distributions to the Gibbs density as \n",
    "$dP= \\frac{1}{Z}e^{T^*}dQ$, where $Z = \\mathbb{E}_{Q}[e^{T^*}]$.\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL}(P||Q)&=\\mathbb{E}_{P}\\log \\frac{dP}{dQ}\\\\\n",
    "&=\\mathbb{E}_{P}\\log \\frac{dP}{dG} + \\mathbb{E}_{P}\\log \\frac{dG}{dQ}\\\\\n",
    "&\\geq \\mathbb{E}_{P}\\log \\frac{dG}{dQ}, \\forall G\n",
    "\\end{align}\n",
    "when $dG = \\frac{1}{Z} e^T dQ$, \n",
    "$$\\mathbb{E}_P \\log \\frac{dG}{dQ}=\\mathbb{E}_P\\log \\left[\\frac{1}{Z}e^T\\right]=\\mathbb{E}_{P}[T]-\\log\\left(\\mathbb{E}_Q [e^T]\\right)$$\n",
    "\n",
    "f-div:\n",
    "$$D_{KL}(P||Q)=\\sup_{T:\\Omega\\rightarrow \\mathbb{R}} \\mathbb{E}_{P}[T]-\\mathbb{E}_{Q}[e^{T-1}]$$\n",
    "DV-Bound is tighter than f-div.\n",
    "\n",
    "Correcting the bias from stochastic gradients.\n",
    "\n",
    "Replacing the estimate in the denominator by an exponential moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gtegner/mine-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     29,
     49,
     62,
     63,
     70,
     75,
     78,
     79,
     88,
     96,
     106,
     108,
     111,
     120,
     129,
     145,
     158,
     165,
     168,
     185,
     193,
     202,
     213,
     224
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from mine.models.gan import GAN\n",
    "\n",
    "from mine.datasets import FunctionDataset, MultivariateNormalDataset\n",
    "from mine.models.layers import ConcatLayer, CustomSequential\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import mine.utils\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "class EMALoss(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, running_ema):\n",
    "        ctx.save_for_backward(input, running_ema)\n",
    "        input_log_sum_exp = input.exp().mean().log()\n",
    "\n",
    "        return input_log_sum_exp\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, running_mean = ctx.saved_tensors\n",
    "        grad = grad_output * input.exp().detach() / \\\n",
    "            (running_mean + EPS) / input.shape[0]\n",
    "        return grad, None\n",
    "\n",
    "\n",
    "def ema(mu, alpha, past_ema):\n",
    "    return alpha * mu + (1.0 - alpha) * past_ema\n",
    "\n",
    "\n",
    "def ema_loss(x, running_mean, alpha):\n",
    "    t_exp = torch.exp(torch.logsumexp(x, 0) - math.log(x.shape[0])).detach()\n",
    "    if running_mean == 0:\n",
    "        running_mean = t_exp\n",
    "    else:\n",
    "        running_mean = ema(t_exp, alpha, running_mean.item())\n",
    "    t_log = EMALoss.apply(x, running_mean)\n",
    "\n",
    "    # Recalculate ema\n",
    "\n",
    "    return t_log, running_mean\n",
    "\n",
    "\n",
    "class Mine(nn.Module):\n",
    "    def __init__(self, T, loss='mine', alpha=0.01, method=None):\n",
    "        super().__init__()\n",
    "        self.running_mean = 0\n",
    "        self.loss = loss\n",
    "        self.alpha = alpha\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'concat':\n",
    "            if isinstance(T, nn.Sequential):\n",
    "                self.T = CustomSequential(ConcatLayer(), *T)\n",
    "            else:\n",
    "                self.T = CustomSequential(ConcatLayer(), T)\n",
    "        else:\n",
    "            self.T = T\n",
    "\n",
    "    def forward(self, x, z, z_marg=None):\n",
    "        if z_marg is None:\n",
    "            z_marg = z[torch.randperm(x.shape[0])]\n",
    "\n",
    "        t = self.T(x, z).mean()\n",
    "        t_marg = self.T(x, z_marg)\n",
    "\n",
    "        if self.loss in ['mine']:\n",
    "            second_term, self.running_mean = ema_loss(\n",
    "                t_marg, self.running_mean, self.alpha)\n",
    "        elif self.loss in ['fdiv']:\n",
    "            second_term = torch.exp(t_marg - 1).mean()\n",
    "        elif self.loss in ['mine_biased']:\n",
    "            second_term = torch.logsumexp(\n",
    "                t_marg, 0) - math.log(t_marg.shape[0])\n",
    "\n",
    "        return -t + second_term\n",
    "\n",
    "    def mi(self, x, z, z_marg=None):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float()\n",
    "        if isinstance(z, np.ndarray):\n",
    "            z = torch.from_numpy(z).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mi = -self.forward(x, z, z_marg)\n",
    "        return mi\n",
    "\n",
    "    def optimize(self, X, Y, iters, batch_size, opt=None):\n",
    "\n",
    "        if opt is None:\n",
    "            opt = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "        for iter in range(1, iters + 1):\n",
    "            mu_mi = 0\n",
    "            for x, y in utils.batch(X, Y, batch_size):\n",
    "                opt.zero_grad()\n",
    "                loss = self.forward(x, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                mu_mi -= loss.item()\n",
    "            if iter % (iters // 3) == 0:\n",
    "                pass\n",
    "                #print(f\"It {iter} - MI: {mu_mi / batch_size}\")\n",
    "\n",
    "        final_mi = self.mi(X, Y)\n",
    "        print(f\"Final MI: {final_mi}\")\n",
    "        return final_mi\n",
    "\n",
    "\n",
    "class T(nn.Module):\n",
    "    def __init__(self, x_dim, z_dim):\n",
    "        super().__init__()\n",
    "        self.layers = CustomSequential(ConcatLayer(), nn.Linear(x_dim + z_dim, 400),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(400, 400),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(400, 400),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(400, 1))\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        return self.layers(x, z)\n",
    "\n",
    "\n",
    "class MutualInformationEstimator(pl.LightningModule):\n",
    "    def __init__(self, x_dim, z_dim, loss='mine', **kwargs):\n",
    "        super().__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.T = CustomSequential(ConcatLayer(), nn.Linear(x_dim + z_dim, 100), nn.ReLU(),\n",
    "                                  nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "\n",
    "        self.energy_loss = Mine(self.T, loss=loss, alpha=kwargs['alpha'])\n",
    "\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        self.train_loader = kwargs.get('train_loader')\n",
    "        self.test_loader = kwargs.get('test_loader')\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        if self.on_gpu:\n",
    "            x = x.cuda()\n",
    "            z = z.cuda()\n",
    "\n",
    "        return self.energy_loss(x, z)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.kwargs['lr'])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, z = batch\n",
    "\n",
    "        if self.on_gpu:\n",
    "            x = x.cuda()\n",
    "            z = z.cuda()\n",
    "\n",
    "        loss = self.energy_loss(x, z)\n",
    "        mi = -loss\n",
    "        tensorboard_logs = {'loss': loss, 'mi': mi}\n",
    "        tqdm_dict = {'loss_tqdm': loss, 'mi': mi}\n",
    "\n",
    "        return {\n",
    "            **tensorboard_logs, 'log': tensorboard_logs, 'progress_bar': tqdm_dict\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, z = batch\n",
    "        loss = self.energy_loss(x, z)\n",
    "\n",
    "        return {\n",
    "            'test_loss': loss, 'test_mi': -loss\n",
    "        }\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        avg_mi = torch.stack([x['test_mi']\n",
    "                              for x in outputs]).mean().detach().cpu().numpy()\n",
    "        tensorboard_logs = {'test_mi': avg_mi}\n",
    "\n",
    "        self.avg_test_mi = avg_mi\n",
    "        return {'avg_test_mi': avg_mi, 'log': tensorboard_logs}\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        if self.train_loader:\n",
    "            return self.train_loader\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            FunctionDataset(self.kwargs['N'], self.x_dim,\n",
    "                            self.kwargs['sigma'], self.kwargs['f']),\n",
    "            batch_size=self.kwargs['batch_size'], shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        if self.test_loader:\n",
    "            return self.train_loader\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            FunctionDataset(self.kwargs['N'], self.x_dim,\n",
    "                            self.kwargs['sigma'], self.kwargs['f']),\n",
    "            batch_size=self.kwargs['batch_size'], shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "\n",
    "def build_dist(rho):\n",
    "    mu = torch.tensor([0.0, 0.0])\n",
    "    cov = torch.tensor([[1, rho], [rho, 1]])\n",
    "    dist = MultivariateNormal(mu, cov)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def function_experiment():\n",
    "    N = 3000\n",
    "    lr = 1e-4\n",
    "    batch_size = 256\n",
    "    epochs = 200\n",
    "\n",
    "    def f1(x): return x\n",
    "    def f2(x): return x**3\n",
    "    def f3(x): return torch.sin(x)\n",
    "    sigmas = torch.linspace(0, 0.9, 10)\n",
    "    fs = [f1, f2, f3]\n",
    "    dim = 2\n",
    "\n",
    "    res = []\n",
    "    for sigma in sigmas:\n",
    "        for ix, f in enumerate(fs):\n",
    "            print(f\"Experiment: {ix + 1}, Sigma: {sigma}...\")\n",
    "\n",
    "            kwargs = {\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'f': f,\n",
    "                'lr': lr,\n",
    "                'batch_size': batch_size\n",
    "            }\n",
    "\n",
    "            model = MutualInformationEstimator(\n",
    "                dim, dim, loss='mine', **kwargs).to(device)\n",
    "            trainer = Trainer(max_epochs=epochs,\n",
    "                              early_stop_callback=False, gpus=1)\n",
    "            trainer.fit(model)\n",
    "            trainer.test()\n",
    "\n",
    "            # Append result\n",
    "            res.append([ix, sigma, model.avg_test_mi])\n",
    "\n",
    "    res = np.array(res)\n",
    "    Z = res[:, -1].reshape((len(sigmas), len(fs))).T\n",
    "    plt.figure()\n",
    "    plt.imshow(Z, cmap='Blues')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def rho_experiment():\n",
    "    dim = 20\n",
    "    N = 3000\n",
    "    lr = 1e-3\n",
    "    epochs = 100\n",
    "    batch_size = 128\n",
    "\n",
    "    x_dim = dim\n",
    "    z_dim = dim\n",
    "\n",
    "    steps = 20\n",
    "    rhos = np.linspace(-0.99, 0.99, steps)\n",
    "    res = []\n",
    "\n",
    "    # Rho Experiment\n",
    "    for rho in rhos:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            MultivariateNormalDataset(N, dim, rho), batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            MultivariateNormalDataset(N, dim, rho), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        true_mi = train_loader.dataset.true_mi\n",
    "\n",
    "        kwargs = {\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'train_loader': train_loader,\n",
    "            'test_loader': test_loader,\n",
    "            'alpha': 1.0\n",
    "        }\n",
    "\n",
    "        model = MutualInformationEstimator(\n",
    "            dim, dim, loss='mine_biased', **kwargs).to(device)\n",
    "        trainer = Trainer(max_epochs=epochs, early_stop_callback=False, gpus=1)\n",
    "        trainer.fit(model)\n",
    "        trainer.test()\n",
    "\n",
    "        print(\"True_mi {}\".format(true_mi))\n",
    "        print(\"MINE {}\".format(model.avg_test_mi))\n",
    "        res.append((rho, model.avg_test_mi, true_mi))\n",
    "\n",
    "    res = np.array(res)\n",
    "    plt.figure()\n",
    "    plt.plot(res[:, 0], res[:, 1], label='MINE')\n",
    "    plt.plot(res[:, 0], res[:, 2], linestyle='--', label='True MI')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gan_experiment():\n",
    "\n",
    "    batch_size = 256\n",
    "    kwargs = {}\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    img, label = next(iter(train_loader))\n",
    "\n",
    "    output_dim = 28*28\n",
    "    input_dim = 100\n",
    "    lr = 2e-3\n",
    "    print_every = 100\n",
    "\n",
    "    mi_model = T(output_dim, 1)\n",
    "\n",
    "    mi_estimator = Mine(mi_model, loss='mine').to(device)\n",
    "    opt_mi = torch.optim.Adam(mi_estimator.parameters(), lr=lr)\n",
    "\n",
    "    model = GAN(input_dim, output_dim, conditional_dim=1,\n",
    "                mi_estimator=mi_estimator, device=device, __lambda__=0.0).to(device)\n",
    "\n",
    "    epochs = 100\n",
    "    opt_g = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    opt_d = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for ix, (img, label) in enumerate(train_loader):\n",
    "\n",
    "            if device == 'cuda':\n",
    "                label = label.float().cuda()\n",
    "                img = img.cuda()\n",
    "\n",
    "            d_loss, generator_loss = model.loss_fn(\n",
    "                img, opt_g, opt_d, opt_mi, conditional=label)\n",
    "\n",
    "            if ix % print_every == 0:\n",
    "                prct = (ix + 2) * batch_size/(batch_size * len(train_loader))\n",
    "                print(\n",
    "                    f\"Epoch {epoch} [{(ix + 2) * batch_size}/{batch_size * len(train_loader)}] [{100*prct:.3}%] Loss (d/g): [{d_loss.item():.3}/{generator_loss.item():.3}]\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rho_experiment()\n",
    "    # function_experiment()\n",
    "    # gan_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
