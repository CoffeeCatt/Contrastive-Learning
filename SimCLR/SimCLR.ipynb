{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11ac975",
   "metadata": {},
   "source": [
    "Data augmentation: plays a critical role in defining effective predictive tasks.\n",
    "\n",
    "Contrastive learning benefits from larger batch size and more training steps.\n",
    "\n",
    "Representation learning with contrastive cross entropy loss benefits from normalized embedding. \n",
    "\n",
    "$$l_{i,j}=-\\log \\frac{\\exp(\\text{sim} (z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{k\\neq i}\\exp(\\text{sim} (z_i, z_j)/\\tau})$$\n",
    "\n",
    "Large batch size optimizer: LARS optimizer\n",
    "\n",
    "augmentation: random cropping and random color distortion.\n",
    "\n",
    "The hidden layer before the projection head is a better representation than the layer after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612eda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def simclr_loss_func(\n",
    "    z1: torch.Tensor,\n",
    "    z2: torch.Tensor,\n",
    "    temperature: float = 0.1,\n",
    "    extra_pos_mask: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computes SimCLR's loss given batch of projected features z1 from view 1 and\n",
    "    projected features z2 from view 2.\n",
    "\n",
    "    Args:\n",
    "        z1 (torch.Tensor): NxD Tensor containing projected features from view 1.\n",
    "        z2 (torch.Tensor): NxD Tensor containing projected features from view 2.\n",
    "        temperature (float): temperature factor for the loss. Defaults to 0.1.\n",
    "        extra_pos_mask (Optional[torch.Tensor]): boolean mask containing extra positives other\n",
    "            than normal across-view positives. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: SimCLR loss.\n",
    "    \"\"\"\n",
    "\n",
    "    device = z1.device\n",
    "\n",
    "    b = z1.size(0)\n",
    "    z = torch.cat((z1, z2), dim=0)\n",
    "    z = F.normalize(z, dim=-1)\n",
    "\n",
    "    logits = torch.einsum(\"if, jf -> ij\", z, z) / temperature\n",
    "    logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "    logits = logits - logits_max.detach()\n",
    "\n",
    "    # positive mask are matches i, j (i from aug1, j from aug2), where i == j and matches j, i\n",
    "    pos_mask = torch.zeros((2 * b, 2 * b), dtype=torch.bool, device=device)\n",
    "    pos_mask[:, b:].fill_diagonal_(True)\n",
    "    pos_mask[b:, :].fill_diagonal_(True)\n",
    "\n",
    "    # if we have extra \"positives\"\n",
    "    if extra_pos_mask is not None:\n",
    "        pos_mask = torch.bitwise_or(pos_mask, extra_pos_mask)\n",
    "\n",
    "    # all matches excluding the main diagonal\n",
    "    logit_mask = torch.ones_like(pos_mask, device=device).fill_diagonal_(0)\n",
    "\n",
    "    exp_logits = torch.exp(logits) * logit_mask\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "    # compute mean of log-likelihood over positives\n",
    "    mean_log_prob_pos = (pos_mask * log_prob).sum(1) / pos_mask.sum(1)\n",
    "    # loss\n",
    "    loss = -mean_log_prob_pos.mean()\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
