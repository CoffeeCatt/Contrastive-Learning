{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiscale==>multiple pos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Philip-Bachman/amdim-public/tree/8754ae149ed28da8066f696f95ba4ca0e3ffebd8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     17
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tanh_clip(x, clip_val=10.):\n",
    "    '''\n",
    "    soft clip values to the range [-clip_val, +clip_val]\n",
    "    '''\n",
    "    if clip_val is not None:\n",
    "        x_clip = clip_val * torch.tanh((1. / clip_val) * x)\n",
    "    else:\n",
    "        x_clip = x\n",
    "    return x_clip\n",
    "\n",
    "\n",
    "def loss_xent(logits, labels, ignore_index=-1):\n",
    "    '''\n",
    "    compute multinomial cross-entropy, for e.g. training a classifier.\n",
    "    '''\n",
    "    xent = F.cross_entropy(tanh_clip(logits, 10.), labels,\n",
    "                           ignore_index=ignore_index)\n",
    "    lgt_reg = 1e-3 * (logits**2.).mean()\n",
    "    return xent + lgt_reg\n",
    "\n",
    "\n",
    "class NCE_MI_MULTI(nn.Module):\n",
    "    def __init__(self, tclip=20.):\n",
    "        super(NCE_MI_MULTI, self).__init__()\n",
    "        self.tclip = tclip\n",
    "\n",
    "    def _model_scores(self, r_src, r_trg, mask_mat):\n",
    "        '''\n",
    "        Compute the NCE scores for predicting r_src->r_trg.\n",
    "        Input:\n",
    "          r_src    : (n_batch_gpu, n_rkhs)\n",
    "          r_trg    : (n_rkhs, n_batch * n_locs)\n",
    "          mask_mat : (n_batch_gpu, n_batch)\n",
    "        Output:\n",
    "          raw_scores : (n_batch_gpu, n_locs)\n",
    "          nce_scores : (n_batch_gpu, n_locs)\n",
    "          lgt_reg    : scalar\n",
    "        '''\n",
    "        n_batch_gpu = mask_mat.size(0)\n",
    "        n_batch = mask_mat.size(1)\n",
    "        n_locs = r_trg.size(1) // n_batch\n",
    "        n_rkhs = r_src.size(1)\n",
    "        # reshape mask_mat for ease-of-use\n",
    "        mask_pos = mask_mat.unsqueeze(dim=2).expand(-1, -1, n_locs).float()\n",
    "        mask_neg = 1. - mask_pos\n",
    "\n",
    "        # compute src->trg raw scores for batch on this gpu\n",
    "        raw_scores = torch.mm(r_src, r_trg).float()\n",
    "        raw_scores = raw_scores.reshape(n_batch_gpu, n_batch, n_locs)\n",
    "        raw_scores = raw_scores / n_rkhs**0.5\n",
    "        lgt_reg = 5e-2 * (raw_scores**2.).mean()\n",
    "        raw_scores = tanh_clip(raw_scores, clip_val=self.tclip)\n",
    "\n",
    "        '''\n",
    "        pos_scores includes scores for all the positive samples\n",
    "        neg_scores includes scores for all the negative samples, with\n",
    "        scores for positive samples set to the min score (-self.tclip here)\n",
    "        '''\n",
    "        # (n_batch_gpu, n_locs)\n",
    "        pos_scores = (mask_pos * raw_scores).sum(dim=1)\n",
    "        # (n_batch_gpu, n_batch, n_locs)\n",
    "        neg_scores = (mask_neg * raw_scores) - (self.tclip * mask_pos)\n",
    "        # (n_batch_gpu, n_batch * n_locs)\n",
    "        neg_scores = neg_scores.reshape(n_batch_gpu, -1)\n",
    "        # (n_batch_gpu, n_batch * n_locs)\n",
    "        mask_neg = mask_neg.reshape(n_batch_gpu, -1)\n",
    "        '''\n",
    "        for each set of positive examples P_i, compute the max over scores\n",
    "        for the set of negative samples N_i that are shared across P_i\n",
    "        '''\n",
    "        # (n_batch_gpu, 1)\n",
    "        neg_maxes = torch.max(neg_scores, dim=1, keepdim=True)[0]\n",
    "        '''\n",
    "        compute a \"partial, safe sum exp\" over each negative sample set N_i,\n",
    "        to broadcast across the positive samples in P_i which share N_i\n",
    "        -- size will be (n_batch_gpu, 1)\n",
    "        '''\n",
    "        neg_sumexp = \\\n",
    "            (mask_neg * torch.exp(neg_scores - neg_maxes)).sum(dim=1, keepdim=True)\n",
    "        '''\n",
    "        use broadcasting of neg_sumexp across the scores in P_i, to compute\n",
    "        the log-sum-exps for the denominators in the NCE log-softmaxes\n",
    "        -- size will be (n_batch_gpu, n_locs)\n",
    "        '''\n",
    "        all_logsumexp = torch.log(torch.exp(pos_scores - neg_maxes) + neg_sumexp)\n",
    "        # compute numerators for the NCE log-softmaxes\n",
    "        pos_shiftexp = pos_scores - neg_maxes\n",
    "        # compute the final log-softmax scores for NCE...\n",
    "        nce_scores = pos_shiftexp - all_logsumexp\n",
    "        return nce_scores, pos_scores, lgt_reg\n",
    "\n",
    "    def _loss_g2l(self, r_src, r_trg, mask_mat):\n",
    "        # compute the nce scores for these features\n",
    "        nce_scores, raw_scores, lgt_reg = \\\n",
    "            self._model_scores(r_src, r_trg, mask_mat)\n",
    "        loss_g2l = -nce_scores.mean()\n",
    "        return loss_g2l, lgt_reg\n",
    "\n",
    "    def forward(self, r1_src_1, r5_src_1, r1_src_2, r5_src_2,\n",
    "                r5_trg_1, r7_trg_1, r5_trg_2, r7_trg_2, mask_mat, mode):\n",
    "        assert(mode in ['train', 'viz'])\n",
    "        if mode == 'train':\n",
    "            # compute values required for visualization\n",
    "            if mask_mat.sum().item() < 1e-1:\n",
    "                # hack for avoiding nce computation on cuda:0\n",
    "                loss_1t5 = mask_mat.detach().mean()\n",
    "                loss_1t7 = mask_mat.detach().mean()\n",
    "                loss_5t5 = mask_mat.detach().mean()\n",
    "                lgt_reg = mask_mat.detach().mean()\n",
    "            else:\n",
    "                # compute costs for 1->5 prediction\n",
    "                loss_1t5_1, lgt_1t5_1 = self._loss_g2l(r1_src_1, r5_trg_2[0], mask_mat)\n",
    "                loss_1t5_2, lgt_1t5_2 = self._loss_g2l(r1_src_2, r5_trg_1[0], mask_mat)\n",
    "                # compute costs for 1->7 prediction\n",
    "                loss_1t7_1, lgt_1t7_1 = self._loss_g2l(r1_src_1, r7_trg_2[0], mask_mat)\n",
    "                loss_1t7_2, lgt_1t7_2 = self._loss_g2l(r1_src_2, r7_trg_1[0], mask_mat)\n",
    "                # compute costs for 5->5 prediction\n",
    "                loss_5t5_1, lgt_5t5_1 = self._loss_g2l(r5_src_1, r5_trg_2[0], mask_mat)\n",
    "                loss_5t5_2, lgt_5t5_2 = self._loss_g2l(r5_src_2, r5_trg_1[0], mask_mat)\n",
    "                # combine costs for optimization\n",
    "                loss_1t5 = 0.5 * (loss_1t5_1 + loss_1t5_2)\n",
    "                loss_1t7 = 0.5 * (loss_1t7_1 + loss_1t7_2)\n",
    "                loss_5t5 = 0.5 * (loss_5t5_1 + loss_5t5_2)\n",
    "                lgt_reg = 0.5 * (lgt_1t5_1 + lgt_1t5_2 + lgt_1t7_1 +\n",
    "                                 lgt_1t7_2 + lgt_5t5_1 + lgt_5t5_2)\n",
    "            return loss_1t5, loss_1t7, loss_5t5, lgt_reg\n",
    "        else:\n",
    "            # compute values to use for visualizations\n",
    "            nce_scores, raw_scores, lgt_reg = \\\n",
    "                self._model_scores(r1_src_1, r7_trg_2[0], mask_mat)\n",
    "            return nce_scores, raw_scores\n",
    "\n",
    "\n",
    "class LossMultiNCE(nn.Module):\n",
    "    '''\n",
    "    Input is fixed as r1_x1, r5_x1, r7_x1, r1_x2, r5_x2, r7_x2.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, tclip=10.):\n",
    "        super(LossMultiNCE, self).__init__()\n",
    "        # initialize the dataparallel nce computer (magic!)\n",
    "        self.nce_func = NCE_MI_MULTI(tclip=tclip)\n",
    "        self.nce_func = nn.DataParallel(self.nce_func)\n",
    "        # construct masks for sampling source features from 5x5 layer\n",
    "        masks_r5 = np.zeros((5, 5, 1, 5, 5))\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                masks_r5[i, j, 0, i, j] = 1\n",
    "        masks_r5 = torch.tensor(masks_r5).type(torch.uint8)\n",
    "        masks_r5 = masks_r5.reshape(-1, 1, 5, 5)\n",
    "        self.masks_r5 = nn.Parameter(masks_r5, requires_grad=False)\n",
    "\n",
    "    def _sample_src_ftr(self, r_cnv, masks):\n",
    "        # get feature dimensions\n",
    "        n_batch = r_cnv.size(0)\n",
    "        n_rkhs = r_cnv.size(1)\n",
    "        if masks is not None:\n",
    "            # subsample from conv-ish r_cnv to get a single vector\n",
    "            mask_idx = torch.randint(0, masks.size(0), (n_batch,))\n",
    "            r_cnv = torch.masked_select(r_cnv, masks[mask_idx])\n",
    "        # flatten features for use as globals in glb->lcl nce cost\n",
    "        r_vec = r_cnv.reshape(n_batch, n_rkhs)\n",
    "        return r_vec\n",
    "\n",
    "    def forward(self, r1_x1, r5_x1, r7_x1, r1_x2, r5_x2, r7_x2):\n",
    "        '''\n",
    "        Compute nce infomax costs for various combos of source/target layers.\n",
    "        Compute costs in both directions, i.e. from/to both images (x1, x2).\n",
    "        rK_x1 are features from source image x1.\n",
    "        rK_x2 are features from source image x2.\n",
    "        '''\n",
    "        # compute feature dimensions\n",
    "        n_batch = int(r1_x1.size(0))\n",
    "        n_rkhs = int(r1_x1.size(1))\n",
    "        # make masking matrix to help compute nce costs\n",
    "        mask_mat = torch.eye(n_batch).cuda()\n",
    "\n",
    "        # sample \"source\" features for glb->lcl predictions\n",
    "        r1_src_1 = self._sample_src_ftr(r1_x1, None)\n",
    "        r5_src_1 = self._sample_src_ftr(r5_x1, self.masks_r5)\n",
    "        r1_src_2 = self._sample_src_ftr(r1_x2, None)\n",
    "        r5_src_2 = self._sample_src_ftr(r5_x2, self.masks_r5)\n",
    "\n",
    "        # before shape: (n_batch, n_rkhs, n_dim, n_dim)\n",
    "        r5_trg_1 = r5_x1.permute(1, 0, 2, 3).reshape(n_rkhs, -1)\n",
    "        r7_trg_1 = r7_x1.permute(1, 0, 2, 3).reshape(n_rkhs, -1)\n",
    "        r5_trg_2 = r5_x2.permute(1, 0, 2, 3).reshape(n_rkhs, -1)\n",
    "        r7_trg_2 = r7_x2.permute(1, 0, 2, 3).reshape(n_rkhs, -1)\n",
    "        # after shape: (n_rkhs, n_batch * n_dim * n_dim)\n",
    "\n",
    "        # compute global->local scores and nce costs via nn.Dataparallel\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        r5_trg_1 = r5_trg_1.unsqueeze(dim=0).expand(n_gpus, -1, -1)\n",
    "        r7_trg_1 = r7_trg_1.unsqueeze(dim=0).expand(n_gpus, -1, -1)\n",
    "        r5_trg_2 = r5_trg_2.unsqueeze(dim=0).expand(n_gpus, -1, -1)\n",
    "        r7_trg_2 = r7_trg_2.unsqueeze(dim=0).expand(n_gpus, -1, -1)\n",
    "\n",
    "        # we're going to hackishly cut mem use on device cuda:0\n",
    "        if n_gpus >= 4:\n",
    "            assert (n_batch % (n_gpus - 1) == 0), 'n_batch: {}, n_gpus: {}'.format(n_batch, n_gpus)\n",
    "            # expand tensors with dummy chunks so cuda:0 can skip compute\n",
    "            chunk_size = n_batch // (n_gpus - 1)\n",
    "            dummy_chunk = torch.zeros_like(r1_src_1[:chunk_size])\n",
    "            r1_src_1 = torch.cat([dummy_chunk, r1_src_1], dim=0)\n",
    "            r5_src_1 = torch.cat([dummy_chunk, r5_src_1], dim=0)\n",
    "            r1_src_2 = torch.cat([dummy_chunk, r1_src_2], dim=0)\n",
    "            r5_src_2 = torch.cat([dummy_chunk, r5_src_2], dim=0)\n",
    "            # ...\n",
    "            dummy_chunk = torch.zeros_like(mask_mat[:chunk_size])\n",
    "            mask_mat = torch.cat([dummy_chunk, mask_mat], dim=0)\n",
    "\n",
    "        # compute nce for multiple infomax costs across multiple GPUs\n",
    "        # -- we do some hacky stuff to minimize compute/mem costs for cuda:0\n",
    "        loss_1t5, loss_1t7, loss_5t5, lgt_reg = \\\n",
    "            self.nce_func(r1_src_1, r5_src_1, r1_src_2, r5_src_2,\n",
    "                          r5_trg_1, r7_trg_1, r5_trg_2, r7_trg_2,\n",
    "                          mask_mat, mode='train')\n",
    "\n",
    "        # adjust cost weight to compensate for hacky skip of cuda:0\n",
    "        if n_gpus >= 4:\n",
    "            rescale = float(n_gpus) / float(n_gpus - 1)\n",
    "            loss_1t5 = rescale * loss_1t5.mean()\n",
    "            loss_1t7 = rescale * loss_1t7.mean()\n",
    "            loss_5t5 = rescale * loss_5t5.mean()\n",
    "            lgt_reg = rescale * lgt_reg.mean()\n",
    "        else:\n",
    "            loss_1t5 = loss_1t5.mean()\n",
    "            loss_1t7 = loss_1t7.mean()\n",
    "            loss_5t5 = loss_5t5.mean()\n",
    "            lgt_reg = lgt_reg.mean()\n",
    "        return loss_1t5, loss_1t7, loss_5t5, lgt_reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
