{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563d22dc",
   "metadata": {},
   "source": [
    "This paper is very much alike the SimCLR paper in a supervised learning framework.\n",
    "\n",
    "Motivation: contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative examples.\n",
    "\n",
    "Hard negative class mining(vs hard negative instance): greedily adds examples to form a batch from a class that violates the constraint with the previously selected classes in the batch.\n",
    "\n",
    "Loss function:\n",
    "$$L(\\{x,x^+, \\{x_i\\}_{i=1}^{N-1}\\})=\\log\\left(1+\\sum_{i=1}^{N-1}\\exp(f^{\\top} f_i-f^{\\top} f^+)\\right)$$\n",
    "\n",
    "similar to the multi-class logistic loss (softmax loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcef4e9",
   "metadata": {},
   "source": [
    "https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/src/pytorch_metric_learning/losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65ce41",
   "metadata": {
    "code_folding": [
     0,
     19
    ]
   },
   "outputs": [],
   "source": [
    "def get_all_pairs_indices(labels, ref_labels=None):\n",
    "    \"\"\"\n",
    "    Given a tensor of labels, this will return 4 tensors.\n",
    "    The first 2 tensors are the indices which form all positive pairs\n",
    "    The second 2 tensors are the indices which form all negative pairs\n",
    "    \"\"\"\n",
    "    if ref_labels is None:\n",
    "        ref_labels = labels\n",
    "    labels1 = labels.unsqueeze(1)\n",
    "    labels2 = ref_labels.unsqueeze(0)\n",
    "    matches = (labels1 == labels2).byte()\n",
    "    diffs = matches ^ 1\n",
    "    if ref_labels is labels:\n",
    "        matches.fill_diagonal_(0)\n",
    "    a1_idx, p_idx = torch.where(matches)\n",
    "    a2_idx, n_idx = torch.where(diffs)\n",
    "    return a1_idx, p_idx, a2_idx, n_idx\n",
    "\n",
    "\n",
    "def convert_to_pairs(indices_tuple, labels):\n",
    "    \"\"\"\n",
    "    This returns anchor-positive and anchor-negative indices,\n",
    "    regardless of what the input indices_tuple is\n",
    "    Args:\n",
    "        indices_tuple: tuple of tensors. Each tensor is 1d and specifies indices\n",
    "                        within a batch\n",
    "        labels: a tensor which has the label for each element in a batch\n",
    "    \"\"\"\n",
    "    if indices_tuple is None:\n",
    "        return get_all_pairs_indices(labels)\n",
    "    elif len(indices_tuple) == 4:\n",
    "        return indices_tuple\n",
    "    else:\n",
    "        a, p, n = indices_tuple\n",
    "        return a, p, a, n\n",
    "\n",
    "\n",
    "def convert_to_pos_pairs_with_unique_labels(indices_tuple, labels):\n",
    "    a, p, _, _ = convert_to_pairs(indices_tuple, labels)\n",
    "    _, unique_idx = np.unique(labels[a].cpu().numpy(), return_index=True)\n",
    "    return a[unique_idx], p[unique_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from ..distances import DotProductSimilarity\n",
    "from ..utils import common_functions as c_f\n",
    "from ..utils import loss_and_miner_utils as lmu\n",
    "from .base_metric_loss_function import BaseMetricLossFunction\n",
    "\n",
    "\n",
    "class NPairsLoss(BaseMetricLossFunction):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.add_to_recordable_attributes(name=\"num_pairs\", is_stat=True)\n",
    "        self.cross_entropy = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def compute_loss(self, embeddings, labels, indices_tuple):\n",
    "        anchor_idx, positive_idx = lmu.convert_to_pos_pairs_with_unique_labels(\n",
    "            indices_tuple, labels\n",
    "        )\n",
    "        self.num_pairs = len(anchor_idx)\n",
    "        if self.num_pairs == 0:\n",
    "            return self.zero_losses()\n",
    "        anchors, positives = embeddings[anchor_idx], embeddings[positive_idx]\n",
    "        targets = c_f.to_device(torch.arange(self.num_pairs), embeddings)\n",
    "        sim_mat = self.distance(anchors, positives)\n",
    "        if not self.distance.is_inverted:\n",
    "            sim_mat = -sim_mat\n",
    "        return {\n",
    "            \"loss\": {\n",
    "                \"losses\": self.cross_entropy(sim_mat, targets),\n",
    "                \"indices\": anchor_idx,\n",
    "                \"reduction_type\": \"element\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_default_distance(self):\n",
    "        return DotProductSimilarity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
