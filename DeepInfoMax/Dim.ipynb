{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x--encoder-->global_view\n",
    "\n",
    "random_local_x--local_encoder-->local_view\n",
    "\n",
    "F-div to compute MINE \n",
    "\n",
    "MINE: DV uses KL, F-div uses JS, JS is more stable than DV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rdevon/DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     14,
     34,
     60,
     96,
     98,
     100,
     105
    ]
   },
   "outputs": [],
   "source": [
    "'''Deep infomax models.\n",
    "'''\n",
    "\n",
    "import copy\n",
    "\n",
    "from cortex.plugins import ModelPlugin\n",
    "import torch\n",
    "\n",
    "from cortex_DIM.functions.dim_losses import donsker_varadhan_loss, infonce_loss, fenchel_dual_loss\n",
    "from cortex_DIM.nn_modules.convnet import Convnet\n",
    "from cortex_DIM.nn_modules.resnet import ResNet\n",
    "from cortex_DIM.nn_modules.mi_networks import MI1x1ConvNet, NopNet\n",
    "\n",
    "\n",
    "def sample_locations(enc, n_samples):\n",
    "    '''Randomly samples locations from localized features.\n",
    "    Used for saving memory.\n",
    "    Args:\n",
    "        enc: Features.\n",
    "        n_samples: Number of samples to draw.\n",
    "    Returns:\n",
    "        torch.Tensor\n",
    "    '''\n",
    "    n_locs = enc.size(2)\n",
    "    batch_size = enc.size(0)\n",
    "    weights = torch.tensor([1. / n_locs] * n_locs, dtype=torch.float)\n",
    "    idx = torch.multinomial(weights, n_samples * batch_size, replacement=True) \\\n",
    "        .view(batch_size, n_samples)\n",
    "    enc = enc.transpose(1, 2)\n",
    "    adx = torch.arange(0, batch_size).long()\n",
    "    enc = enc[adx[:, None], idx].transpose(1, 2)\n",
    "    return enc\n",
    "\n",
    "\n",
    "def compute_dim_loss(l_enc, m_enc, measure, mode):\n",
    "    '''Computes DIM loss.\n",
    "    Args:\n",
    "        l_enc: Local feature map encoding.\n",
    "        m_enc: Multiple globals feature map encoding.\n",
    "        measure: Type of f-divergence. For use with mode `fd`\n",
    "        mode: Loss mode. Fenchel-dual `fd`, NCE `nce`, or Donsker-Vadadhan `dv`.\n",
    "    Returns:\n",
    "        torch.Tensor: Loss.\n",
    "    '''\n",
    "\n",
    "    if mode == 'fd':\n",
    "        loss = fenchel_dual_loss(l_enc, m_enc, measure=measure)\n",
    "    elif mode == 'nce':\n",
    "        loss = infonce_loss(l_enc, m_enc)\n",
    "    elif mode == 'dv':\n",
    "        loss = donsker_varadhan_loss(l_enc, m_enc)\n",
    "    else:\n",
    "        raise NotImplementedError(mode)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class GlobalDIM(ModelPlugin):\n",
    "    '''Global version of Deep InfoMax\n",
    "    '''\n",
    "    defaults = dict(\n",
    "        data=dict(batch_size=dict(train=64, test=64),\n",
    "                  inputs=dict(inputs='data.images'), skip_last_batch=True),\n",
    "        train=dict(save_on_lowest='losses.encoder', epochs=1000),\n",
    "        optimizer=dict(learning_rate=1e-4)\n",
    "    )\n",
    "\n",
    "    def build(self, encoder, config, task_idx=None, mi_units=2048):\n",
    "        '''\n",
    "        Args:\n",
    "            task_idx (tuple): Indices where to do local objective.\n",
    "            mi_units (int): Number of units for MI estimation.\n",
    "        '''\n",
    "\n",
    "        self.nets.encoder = encoder\n",
    "\n",
    "        if task_idx is not None:\n",
    "            self.task_idx = task_idx\n",
    "        elif 'local_task_idx' not in config.keys():\n",
    "            raise ValueError('')\n",
    "        else:\n",
    "            self.task_idx = config['local_task_idx']\n",
    "\n",
    "        local_idx, global_idx = self.task_idx\n",
    "\n",
    "        # Create MI nn_modules.\n",
    "        X = self.inputs('data.images')\n",
    "        outs = self.nets.encoder(X, return_all_activations=True)\n",
    "        L, G = [outs[i] for i in self.task_idx]\n",
    "        local_size = L.size()[1:]\n",
    "        global_size = G.size()[1:]\n",
    "\n",
    "        # For global DIM, we'll just copy the layer hyperparameters for the encoder.\n",
    "        layers = copy.deepcopy(config['layers'])\n",
    "        layers[-1] = dict(layer='linear', args=(mi_units,))\n",
    "\n",
    "        if isinstance(encoder.module.encoder, ResNet):\n",
    "            Encoder = ResNet\n",
    "        elif isinstance(encoder.module.encoder, Convnet):\n",
    "            Encoder = Convnet\n",
    "        else:\n",
    "            raise NotImplementedError('Can\\'t do {} yet (feature request)'.format(type(encoder.encoder)))\n",
    "\n",
    "        if len(local_size) == 1 or len(local_size) == 3:\n",
    "            local_MINet = Encoder(local_size[::-1], layers=layers[local_idx:])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if len(global_size) == 1:\n",
    "            if global_size[0] == mi_units:\n",
    "                global_MINet = NopNet()\n",
    "            else:\n",
    "                global_MINet = Encoder(global_size, layers=layers[global_idx:])\n",
    "        elif len(global_size) == 3:\n",
    "            if (global_size[1] == global_size[1] == 1) and global_size[0] == mi_units:\n",
    "                global_MINet = NopNet()\n",
    "            else:\n",
    "                global_MINet = Encoder(global_size, layers=layers[global_idx:])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        local_MINet = local_MINet.to(X.device)\n",
    "        global_MINet = global_MINet.to(X.device)\n",
    "\n",
    "        def extract(outs, local_net=None, global_net=None):\n",
    "            '''Wrapper function to be put in encoder forward for speed.\n",
    "            Args:\n",
    "                outs (list): List of activations\n",
    "                local_net (nn.Module): Network to encode local activations.\n",
    "                global_net (nn.Module): Network to encode global activations.\n",
    "            Returns:\n",
    "                tuple: local, global outputs\n",
    "            '''\n",
    "            l_idx, g_idx = self.task_idx\n",
    "            L = outs[l_idx]\n",
    "            G = outs[g_idx]\n",
    "\n",
    "            L = local_net(L)\n",
    "            G = global_net(G)\n",
    "\n",
    "            N, local_units = L.size()[:2]\n",
    "            L = L.view(N, local_units, -1)\n",
    "            G = G.view(N, local_units, -1)\n",
    "\n",
    "            return L, G\n",
    "\n",
    "        self.nets.encoder.module.add_network(self.name, extract,\n",
    "                                             networks=dict(local_net=local_MINet, global_net=global_MINet))\n",
    "\n",
    "    def routine(self, outs=None, measure='JSD', mode='fd', scale=1.0, act_penalty=0.):\n",
    "        '''\n",
    "        Args:\n",
    "            measure: Type of f-divergence. For use with mode `fd`\n",
    "            mode: Loss mode. Fenchel-dual `fd`, NCE `nce`, or Donsker-Vadadhan `dv`.\n",
    "            scale: Hyperparameter for global DIM. Called `alpha` in the paper.\n",
    "            act_penalty: L2 penalty on the global activations.\n",
    "        '''\n",
    "        L, G = outs[self.name]\n",
    "\n",
    "        act_loss = (G ** 2).sum(1).mean()\n",
    "\n",
    "        loss = compute_dim_loss(L, G, measure, mode)\n",
    "\n",
    "        if scale > 0:\n",
    "            self.add_losses(encoder=scale * loss + act_penalty * act_loss)\n",
    "\n",
    "\n",
    "class LocalDIM(ModelPlugin):\n",
    "    '''Local version of Deep InfoMax\n",
    "    '''\n",
    "\n",
    "    defaults = dict(\n",
    "        data=dict(batch_size=dict(train=64, test=64),\n",
    "                  inputs=dict(inputs='data.images'), skip_last_batch=True),\n",
    "        train=dict(save_on_lowest='losses.encoder', epochs=1000),\n",
    "        optimizer=dict(learning_rate=1e-4)\n",
    "    )\n",
    "\n",
    "    def build(self, encoder, config, task_idx=None, mi_units=2048, global_samples=None, local_samples=None):\n",
    "        '''\n",
    "        Args:\n",
    "            global_units: Number of global units.\n",
    "            task_idx (tuple): Indices where to do local objective.\n",
    "            mi_units: Number of units for MI estimation.\n",
    "            global_samples: Number of samples over the global locations for each example.\n",
    "            local_samples: Number of samples over the local locations for each example.\n",
    "        '''\n",
    "        self.nets.encoder = encoder\n",
    "\n",
    "        if task_idx is not None:\n",
    "            self.task_idx = task_idx\n",
    "        elif 'local_task_idx' not in config.keys():\n",
    "            raise ValueError('No task_idx provided for local task.')\n",
    "        else:\n",
    "            self.task_idx = config['local_task_idx']\n",
    "\n",
    "        # Create MI nn_modules.\n",
    "        X = self.inputs('data.images')\n",
    "        outs = self.nets.encoder(X, return_all_activations=True)\n",
    "        L, G = [outs[i] for i in self.task_idx]\n",
    "        local_size = L.size()[1:]\n",
    "        global_size = G.size()[1:]\n",
    "\n",
    "        if len(local_size) == 1 or len(local_size) == 3:\n",
    "            local_MINet = MI1x1ConvNet(local_size[0], mi_units)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if len(global_size) == 1:\n",
    "            if global_size[0] == mi_units:\n",
    "                global_MINet = NopNet()\n",
    "            else:\n",
    "                global_MINet = MI1x1ConvNet(global_size[0], mi_units)\n",
    "        elif len(global_size) == 3:\n",
    "            if (global_size[1] == global_size[1] == 1) and global_size[0] == mi_units:\n",
    "                global_MINet = NopNet()\n",
    "            else:\n",
    "                global_MINet = MI1x1ConvNet(global_size[0], mi_units)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        local_MINet = local_MINet.to(X.device)\n",
    "        global_MINet = global_MINet.to(X.device)\n",
    "\n",
    "        def extract(outs, local_net=None, global_net=None):\n",
    "            '''Wrapper function to be put in encoder forward for speed.\n",
    "            Args:\n",
    "                outs (list): List of activations\n",
    "                local_net (nn.Module): Network to encode local activations.\n",
    "                global_net (nn.Module): Network to encode global activations.\n",
    "            Returns:\n",
    "                tuple: local, global outputs\n",
    "            '''\n",
    "            l_idx, g_idx = self.task_idx\n",
    "            L = outs[l_idx]\n",
    "            G = outs[g_idx]\n",
    "\n",
    "            # All globals are reshaped as 1x1 feature maps.\n",
    "            global_size = G.size()[1:]\n",
    "            if len(global_size) == 1:\n",
    "                G = G[:, :, None, None]\n",
    "\n",
    "            L = local_net(L)\n",
    "            G = global_net(G)\n",
    "\n",
    "            N, local_units = L.size()[:2]\n",
    "            L = L.view(N, local_units, -1)\n",
    "            G = G.view(N, local_units, -1)\n",
    "\n",
    "            # Sample locations for saving memory.\n",
    "            if global_samples is not None:\n",
    "                G = sample_locations(G, global_samples)\n",
    "\n",
    "            if local_samples is not None:\n",
    "                L = sample_locations(L, local_samples)\n",
    "\n",
    "            return L, G\n",
    "\n",
    "        self.nets.encoder.module.add_network(self.name, extract,\n",
    "                                             networks=dict(local_net=local_MINet, global_net=global_MINet))\n",
    "\n",
    "    def routine(self, outs=None, measure='JSD', mode='fd', scale=1.0, act_penalty=0.):\n",
    "        '''\n",
    "        Args:\n",
    "            measure: Type of f-divergence. For use with mode `fd`.\n",
    "            mode: Loss mode. Fenchel-dual `fd`, NCE `nce`, or Donsker-Vadadhan `dv`.\n",
    "            scale: Hyperparameter for local DIM. Called `beta` in the paper.\n",
    "            act_penalty: L2 penalty on the global activations. Can improve stability.\n",
    "        '''\n",
    "        L, G = outs[self.name]\n",
    "\n",
    "        if act_penalty > 0.:\n",
    "            act_loss = act_penalty * (G ** 2).sum(1).mean()\n",
    "        else:\n",
    "            act_loss = 0.\n",
    "\n",
    "        loss = compute_dim_loss(L, G, measure, mode)\n",
    "\n",
    "        if scale > 0:\n",
    "            self.add_losses(encoder=scale * loss + act_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
